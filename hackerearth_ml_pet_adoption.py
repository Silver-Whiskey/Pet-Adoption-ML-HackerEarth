# -*- coding: utf-8 -*-
"""HackerEarth_ML_Pet_Adoption.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tDUIDHnBAXRuTzSJJVN1LgljEDxLFRVe

**NOTE: This is not the exact solution - individual cells have been used in random order** *italicized text*

## Setup + dataset imports
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

train_df = pd.read_csv('/content/drive/My Drive/HackerEarth Machine Learning Pet Adoption/Dataset/train.csv',parse_dates=True)
test_df = pd.read_csv('/content/drive/My Drive/HackerEarth Machine Learning Pet Adoption/Dataset/test.csv',parse_dates=True)

"""## Data Preprocessing"""

train_df['pet_id'] = train_df['pet_id'].apply(lambda x: x.strip('ANSL_')).astype('int')
test_df['pet_id'] = test_df['pet_id'].apply(lambda x: x.strip('ANSL_')).astype('int')

train_df['issue_date'] = train_df['issue_date'].apply(lambda x: x.strip(' 00:00:00	'))
test_df['issue_date'] = test_df['issue_date'].apply(lambda x: x.strip(' 00:00:00	'))

train_df['issue_date'] = pd.to_datetime(train_df['issue_date'])
test_df['issue_date'] = pd.to_datetime(test_df['issue_date'])

train_df.head()

train_df.info()

train_df['listing_date'] = pd.to_datetime(train_df['listing_date'],utc=True)
test_df['listing_date'] = pd.to_datetime(test_df['listing_date'],utc=True)

train_df['breed_category'] = train_df['breed_category'].astype('int')
#test_df['breed_category'] = test_df['breed_category'].astype('int')

test_df['issue_date']= pd.to_datetime(test_df['issue_date']).astype(int)
train_df['issue_date'] = pd.to_datetime(train_df['issue_date']).astype(int)

test_df['listing_date']= pd.to_datetime(test_df['listing_date']).astype(int)
train_df['listing_date'] = pd.to_datetime(train_df['listing_date']).astype(int)

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
train_df['color_type'] = pd.DataFrame(encoder.fit_transform(pd.DataFrame(train_df['color_type'])))
test_df['color_type'] = pd.DataFrame(encoder.transform(pd.DataFrame(test_df['color_type'])))

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='most_frequent')
imputed_train_df = train_df
imputed_test_df = test_df
imputed_train_df['condition'] = pd.DataFrame(imputer.fit_transform(pd.DataFrame(imputed_train_df['condition'])))
imputed_test_df['condition'] = pd.DataFrame(imputer.transform(pd.DataFrame(test_df['condition'])))

imputed_train_df['condition'] = imputed_train_df['condition'].astype('int')
imputed_test_df['condition'] = imputed_test_df['condition'].astype('int')

"""# Data Copies for Feature Engineering"""

train_df_copy = train_df.copy()
test_df_copy = test_df.copy()

train_df_copy['stayed_in_shelter'] = train_df_copy['listing_date']-train_df_copy['issue_date']
test_df_copy['stayed_in_shelter'] = test_df_copy['listing_date']-test_df_copy['issue_date']

train_df_copy_1 = train_df_copy.copy()
test_df_copy_1 = test_df_copy.copy()
train_df_copy_2 = train_df_copy.copy()
test_df_copy_2 = test_df_copy.copy()

train_df_copy_1['size'] = (train_df_copy_1['length(m)']**2)*train_df_copy_1['height(cm)']/100
test_df_copy_1['size'] = (test_df_copy_1['length(m)']**2)*test_df_copy_1['height(cm)']/100

train_df_copy_2['size'] = (train_df_copy_2['length(m)'])*train_df_copy_2['height(cm)']/100
test_df_copy_2['size'] = (test_df_copy_2['length(m)'])*test_df_copy_2['height(cm)']/100

"""## Dropping for a while"""

train_df_copy_1['stayed_in_shelter'] = train_df_copy_1['listing_date']-train_df_copy_1['issue_date']
test_df_copy_1['stayed_in_shelter'] = test_df_copy_1['listing_date']-test_df_copy_1['issue_date']
train_df_copy_2['stayed_in_shelter'] = train_df_copy_2['listing_date']-train_df_copy_2['issue_date']
test_df_copy_2['stayed_in_shelter'] = test_df_copy_2['listing_date']-test_df_copy_2['issue_date']

Y1 = train_df_copy_1['breed_category']
Y2 = train_df_copy_1['pet_category']
X_train = train_df_copy_1.drop(columns=['breed_category','pet_category'])
X_test = test_df_copy_1

"""# Arranging Data as features and labels"""

Y1 = train_df_copy_1['breed_category']
Y2 = train_df_copy_1['pet_category']
X_train_1 = train_df_copy_1.drop(columns=['breed_category','pet_category'])
X_test_1 = test_df_copy_1
X_train_2 = train_df_copy_2.drop(columns=['breed_category','pet_category'])
X_test_2 = test_df_copy_2

"""# Experimentation:"""

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
print(cross_val_score(model,imputed_X_train_1,Y1,cv=5))
print(cross_val_score(model,imputed_X_train_2,Y1,cv=5))
print(cross_val_score(model,imputed_X_train_1,Y2,cv=5))
print(cross_val_score(model,imputed_X_train_2,Y2,cv=5))

from xgboost import XGBClassifier

model = XGBClassifier()
#print(size = )
print(cross_val_score(model,X_train_1,Y1,cv=5))
print(cross_val_score(model,X_train_2,Y1,cv=5))
print(cross_val_score(model,X_train_1,Y2,cv=5))
print(cross_val_score(model,X_train_2,Y2,cv=5))

import lightgbm as lgb

model = lgb.LGBMClassifier()
print(cross_val_score(model,X_train_1,Y1,cv=5))
print(cross_val_score(model,X_train_2,Y1,cv=5))
print(cross_val_score(model,X_train_1,Y2,cv=5))
print(cross_val_score(model,X_train_2,Y2,cv=5))

"""# Not Worth using an imputer explicitely"""

from sklearn.impute import KNNImputer
imputed_X_train_1 = X_train_1
imputed_X_test_1 = X_test_1
imputed_X_train_2 = X_train_2
imputed_X_test_2 = X_test_2
imputer = KNNImputer()
imputed_X_train_1['condition'] = pd.DataFrame(imputer.fit_transform(pd.DataFrame(X_train_1['condition'])))
imputed_X_test_1['condition'] = pd.DataFrame(imputer.transform(pd.DataFrame(X_test_1['condition'])))
imputed_X_train_2['condition'] = pd.DataFrame(imputer.fit_transform(pd.DataFrame(X_train_2['condition'])))
imputed_X_test_2['condition'] = pd.DataFrame(imputer.transform(pd.DataFrame(X_test_2['condition'])))
imputed_X_train_1.columns = X_train_1.columns
imputed_X_test_1.columns = X_test_1.columns
imputed_X_train_2.columns = X_train_2.columns
imputed_X_test_2.columns = X_test_2.columns

imputed_X_train_1['condition'] = imputed_X_train_1['condition'].astype('int')
imputed_X_test_1['condition'] = imputed_X_test_1['condition'].astype('int')
imputed_X_train_2['condition'] = imputed_X_train_2['condition'].astype('int')
imputed_X_test_2['condition'] = imputed_X_test_2['condition'].astype('int')

X_train_1.info()

from sklearn.ensemble import RandomForestClassifier
#lasso = linear_model.Lasso()
model = RandomForestClassifier()
print(cross_val_score(model,X_train_1,Y1,cv=5))
print(cross_val_score(model,X_train_2,Y1,cv=5))
print(cross_val_score(model,X_train_1,Y2,cv=5))
print(cross_val_score(model,X_train_2,Y2,cv=5))

from xgboost import XGBClassifier

model = XGBClassifier()
print(cross_val_score(model,X_train_1,Y1,cv=5))
print(cross_val_score(model,X_train_2,Y1,cv=5))
print(cross_val_score(model,X_train_1,Y2,cv=5))
print(cross_val_score(model,X_train_2,Y2,cv=5))

import lightgbm as lgb

model = lgb.LGBMClassifier()
print(cross_val_score(model,X_train_1,Y1,cv=5))
print(cross_val_score(model,X_train_2,Y1,cv=5))
print(cross_val_score(model,X_train_1,Y2,cv=5))
print(cross_val_score(model,X_train_2,Y2,cv=5))

"""# Final_preds"""

model1 = XGBClassifier()
model1.fit(X_train,Y1)
preds_1 = model1.predict(X_test)

import lightgbm as lgb
model2 = lgb.LGBMClassifier()
model2.fit(X_train,Y2)
preds_2 = model2.predict(X_test)

test_submit = pd.read_csv('/content/drive/My Drive/HackerEarth Machine Learning Pet Adoption/Dataset/test.csv')
test_pet_ids = pd.DataFrame(test_submit['pet_id'])

test_pet_ids['breed_category'] = preds_1
test_pet_ids['pet_category'] = preds_2

test_pet_ids.head()

test_pet_ids.to_csv("pred_5_xgb_lgb.csv",index=False)

#Data_Viz